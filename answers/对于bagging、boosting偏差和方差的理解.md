# 对于 bagging、boosting 偏差和方差的理解

## 偏差和方差

偏差衡量了模型的预测值与实际值之间的偏离关系。
方差描述的是训练数据在不同迭代阶段的训练模型中，预测值的变化波动情况（或称之为离散情况）。
详见[偏差和方差](https://github.com/buki26/InterviewQuestions/blob/master/answers/%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE.md)

插一句，关于强分类器和弱分类器，能够迅速正确的识别的过程就是强分类器，而易错的则是弱分类器。

集成算法的本质是把弱分类器集成起来成为一个强分类器。
对于弱分类器而言，直观的理解，由于分类能力有限，它的偏差大，方差小。
方差小是有好处的，我们认为方差越大的模型越容易过拟合：假设有两个训练集 A 和 B，经过 A 训练的模型 Fa 与经过 B 训练的模型 Fb 差异很大，这意味着 Fa 在类 A 的样本集合上有更好的性能，而 Fb 在类 B 的训练样本集合上有更好的性能，这样导致在不同的训练集样本的条件下，训练得到的模型的效果差异性很大，很不稳定，这便是模型的过拟合现象，而对于一些弱模型，它在不同的训练样本集上性能差异并不大，因此模型方差小，抗过拟合能力强。

## bagging

同质集成算法，典型的 bagging 算法是随机森林。
原理是通过弱分类器的集成，有放回的抽样。bagging 算法中的每个基模型的权重是 1/m，它整体的方差是小于等于基模型方差的，也就是说，bagging 算法主要关注降低方差，增加模型的稳定性（抗过拟合能力强）。

## boosting

boosting 也是同质集成，它可以将弱学习器提升为强学习器，典型的算法有 AdaBoost 和 GBDT 等。
不同于 bagging 的并行执行，boosting 是串行完成的，即下一个弱分类器根据前一个弱分类器的表现，调整权重，重点关注前一个弱分类器分错的样本。
如 AdaBoost 中有两种权重，一种是样本的权重（在下一个分类器训练时，使得上一个分类器表现不好的样本的权重更大，即重点训练），另一种是弱分类器的权重（表现好的弱分类器在最终模型中的权重大）。
boosting 算法更关注降低偏差，相对于弱分类器，精度会有显著提升，但有过拟合风险。

## 总结

bagging，以随机森林为代表，高偏差，低方差，抗过拟合能力强。
boosting，以 AdaBoost 为代表，低偏差，高方差，准确率高，但有过拟合风险。
