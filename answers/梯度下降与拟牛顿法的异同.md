# 梯度下降与拟牛顿法的异同

都属于常见的优化算法，用于机器学习算法中，通过最小化目标函数来训练出最合适的模型。

## 梯度下降法

梯度下降法的基本思想是，用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是“最速下降法”。最速下降法越接近目标值，步长越小，前进越慢。
![](https://github.com/buki26/image/blob/master/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95.png?raw=true)
对于梯度下降法，比较直观的理解就是，在山坡上任意位置，要走向山脚的最快速度，就是找到当前位置下山最陡的方向，朝这个方向走一小步，然后重新定位最陡的方向，重复这个步骤，直到下降到最底。

梯度下降法的缺点：
（1）靠近极小值时收敛速度减慢，如下图所示；
（2）直线搜索时可能会产生一些问题；
（3）可能会“之字形”地下降。

### 批量梯度下降法（Batch Gradient Descent）

梯度下降法最常用的形式，做法是在更新参数时，也就是上面提到的每次重新定位最陡的方向时，使用所有的样本来更新参数。
缺点就是，样本量很大的时候，每求一次梯度就需要用上所有的样本，计算量比较大

### 随机梯度下降法（Stochastic Gradient Descent）

随机梯度下降法与上面的 BGD 的区别在于，更新参数时不使用全部样本，而是使用其中一个样本来更新参数。这样做在样本量很大的时候，大大提高了速度，但是在准确率方面，由于只使用了一个样本，得到的解有可能不是最优解。

### 小批量梯度下降法（Mini-batch Gradient Descent）

上面两种方法的一个折中方案，采用部分样本来更新参数。

## 牛顿法

牛顿法使用函数 f(x)的泰勒级数的前面几项来寻找方程 f(x) = 0 的根。牛顿法最大的特点就在于它的收敛速度很快。
牛顿法示意图：
![](https://github.com/buki26/image/blob/master/%E7%89%9B%E9%A1%BF%E6%B3%951.jpg?raw=true)
![](https://github.com/buki26/image/blob/master/%E7%89%9B%E9%A1%BF%E6%B3%952.jpg?raw=true)
![](https://github.com/buki26/image/blob/master/%E7%89%9B%E9%A1%BF%E6%B3%953.jpg?raw=true)
![](https://github.com/buki26/image/blob/master/%E7%89%9B%E9%A1%BF%E6%B3%954.jpg?raw=true)

**关于牛顿法和梯度下降法的效率对比**

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

对比示意图：
![](https://github.com/buki26/image/blob/master/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E7%89%9B%E9%A1%BF%E6%B3%95%E5%AF%B9%E6%AF%94.jpg?raw=true)
牛顿法的缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的 Hessian 矩阵的逆矩阵，计算比较复杂。
解决方案：拟牛顿法

## 拟牛顿法

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的 Hessian 矩阵的逆矩阵的缺陷，它使用正定矩阵来近似 Hessian 矩阵的逆，从而简化了运算的复杂度。
