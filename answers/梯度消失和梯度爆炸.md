# 梯度消失和梯度爆炸

## 概念

在深度学习网络中，采用梯度下降法来做损失函数的最小化，从而计算合适的权重。因为深度网络可以视为是一个复合的非线性多元函数，在用梯度下降法求取损失函数最小的参数权重过程中，需要对非线性多元函数求偏导，通过反向传播算法更新权重的过程中，得到多个偏导相乘的值用于权重更新。
![](https://github.com/buki26/image/blob/master/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%9D%83%E9%87%8D%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B-%E6%B1%82%E5%81%8F%E5%AF%BC.png?raw=true)
梯度消失：此部分小于 1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了梯度消失。
梯度爆炸：此部分大于 1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生梯度爆炸。

## 原因

从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。因此，梯度消失、爆炸，其根本原因在于反向传播训练法则，属于先天不足。
从激活函数看：
1、 sigmoid 用作激活函数
函数图像及梯度图像：
![](https://github.com/buki26/image/blob/master/sigmoid%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png?raw=true)
可以看出，用 sigmoid 做激活函数梯度是不可能超过 0.25 的，这样经过链式求导之后，很容易发生梯度消失。
2、 tanh
![](https://github.com/buki26/image/blob/master/tahn%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png?raw=true)
tanh 比 sigmoid 要好一些，但是它的导数仍然是小于 1 的。

## 解决方案

用 relu、leakrelu、elu 等激活函数
1、 relu
![](https://github.com/buki26/image/blob/master/relu%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png?raw=true)
仍存在的问题：由于负数部分恒为 0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
2、 leakrelu
![](https://github.com/buki26/image/blob/master/leakrelu%E5%87%BD%E6%95%B0.png?raw=true)
3、 elu
![](https://github.com/buki26/image/blob/master/elu%E5%87%BD%E6%95%B0%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0.png?raw=true)
